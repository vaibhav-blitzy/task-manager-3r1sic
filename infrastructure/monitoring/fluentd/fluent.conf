# Fluentd Configuration for Task Management System
# Version: 1.0.0
# =================================================================
# This configuration defines how logs are collected, processed, and 
# routed to Elasticsearch, AlertManager, and S3 for archiving.
# 
# Required plugins:
# - fluent-plugin-elasticsearch (v5.1.0)
# - fluent-plugin-s3 (v1.6.0)
# - fluent-plugin-prometheus (v2.0.0)
# - fluent-plugin-rewrite-tag-filter (v2.4.0)
# =================================================================

# Include system-wide configuration
@include system.conf

# Include plugin-specific configurations
@include plugins/*.conf

# =================================================================
# SYSTEM CONFIGURATION
# =================================================================
<system>
  log_level info
  workers 4
  root_dir /var/log/fluentd
  
  # System-wide buffer configurations
  <buffer_config>
    flush_interval 5s
    flush_thread_count 4
    chunk_limit_size 8m
    total_limit_size 4g
    overflow_action block
  </buffer_config>
  
  # Log rotation for Fluentd's internal logs
  <log>
    format json
    time_format %Y-%m-%dT%H:%M:%S%z
    rotate_age 5
    rotate_size 10485760 # 10MB
  </log>
</system>

# Expose metrics for Prometheus monitoring
<source>
  @type prometheus
  bind 0.0.0.0
  port 24231
  metrics_path /metrics
</source>

# =================================================================
# INPUT SOURCES
# =================================================================
# Primary source - Receive logs from Fluent Bit agents
<source>
  @type forward
  bind 0.0.0.0
  port 24224
  
  # TLS configuration for secure communication
  <transport tls>
    cert_path /etc/fluentd/certs/fluentd.crt
    private_key_path /etc/fluentd/certs/fluentd.key
    client_cert_auth true
    ca_path /etc/fluentd/certs/ca.crt
  </transport>
  
  # Authentication
  <security>
    self_hostname fluentd.monitoring.svc
    shared_key "#{ENV['FLUENTD_SHARED_KEY']}"
  </security>
  
  # Record the source of logs
  <inject>
    time_key time
    time_type string
    time_format %Y-%m-%dT%H:%M:%S%z
    tag_key tag
  </inject>
</source>

# =================================================================
# TAG REWRITING AND ROUTING
# =================================================================
# Rewrite tags based on log type for proper routing
<match kubernetes.**>
  @type rewrite_tag_filter
  
  # Application logs
  <rule>
    key $.kubernetes.labels.app
    pattern /^(?<service>.+)$/
    tag app.${service}
  </rule>
  
  # Access logs (identified by nginx, kong, or api-gateway labels)
  <rule>
    key $.kubernetes.labels.component
    pattern /^(nginx|kong|api-gateway)$/
    tag access.${tag}
  </rule>
  
  # Audit logs
  <rule>
    key $.kubernetes.labels.log-type
    pattern /^audit$/
    tag audit.${tag}
  </rule>
  
  # System logs
  <rule>
    key $.kubernetes.container_name
    pattern /^(fluentd|prometheus|node-exporter|kube-.*)$/
    tag system.${tag}
  </rule>
  
  # Default - catch any unlabeled logs
  <rule>
    key $.kubernetes.namespace_name
    pattern /.+/
    tag app.default.${tag}
  </rule>
</match>

# =================================================================
# LOG PROCESSING FILTERS
# =================================================================
# Common processing for all logs
<filter **>
  @type record_transformer
  enable_ruby true
  
  # Add environment metadata
  <record>
    environment "#{ENV['FLUENTD_ENV'] || 'production'}"
    fluentd_hostname "#{Socket.gethostname}"
    timestamp ${time.strftime('%Y-%m-%dT%H:%M:%S.%L%z')}
  </record>
</filter>

# Parse JSON logs
<filter app.**>
  @type parser
  key_name log
  reserve_data true
  <parse>
    @type json
    time_key time
    time_format %Y-%m-%dT%H:%M:%S.%L%z
  </parse>
</filter>

# Process application logs
<filter app.**>
  @type record_transformer
  enable_ruby true
  
  # Normalize log structure
  <record>
    severity ${record["level"] || record["severity"] || record["log_level"] || "info"}
    service_name ${tag_parts[1]}
    message ${record["message"] || record["msg"] || record["log"]}
    correlation_id ${record["correlation_id"] || record["request_id"] || ""}
    user_id ${record["user_id"] || ""}
  </record>
  
  # PII redaction for sensitive fields
  <record>
    message ${record["message"].gsub(/\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Za-z]{2,}\b/, '[REDACTED_EMAIL]') if record["message"].is_a?(String)}
  </record>
</filter>

# Special processing for access logs
<filter access.**>
  @type parser
  key_name log
  reserve_data true
  <parse>
    @type nginx
  </parse>
</filter>

<filter access.**>
  @type record_transformer
  <record>
    http_method ${record["method"]}
    http_path ${record["path"]}
    http_status ${record["code"]}
    response_time_ms ${record["request_time"] ? (record["request_time"].to_f * 1000).to_i : 0}
    remote_ip ${record["remote"]}
    user_agent ${record["agent"]}
  </record>
</filter>

# Special processing for audit logs
<filter audit.**>
  @type record_transformer
  <record>
    audit_event ${record["event"] || "unknown"}
    actor ${record["user"] || record["actor"] || "system"}
    resource_type ${record["resource_type"] || ""}
    resource_id ${record["resource_id"] || ""}
    action ${record["action"] || ""}
    outcome ${record["outcome"] || "unknown"}
  </record>
</filter>

# =================================================================
# ELASTICSEARCH OUTPUT
# =================================================================
# Application logs to Elasticsearch
<match app.**>
  @type elasticsearch
  host elasticsearch.monitoring.svc
  port 9200
  user "#{ENV['ES_USERNAME']}"
  password "#{ENV['ES_PASSWORD']}"
  scheme https
  ssl_verify true
  ssl_version TLSv1_2
  
  logstash_format true
  logstash_prefix app-logs
  include_tag_key true
  tag_key @log_name
  
  <buffer>
    @type file
    path /var/log/fluentd/buffer/app-es
    flush_interval 5s
    retry_max_interval 30s
    retry_forever false
    retry_max_times 17  # Retry for ~5 hours
    flush_thread_count 4
  </buffer>
  
  <secondary>
    @type file
    path /var/log/fluentd/failed/app-es
    compress gzip
  </secondary>
</match>

# Access logs to Elasticsearch
<match access.**>
  @type elasticsearch
  host elasticsearch.monitoring.svc
  port 9200
  user "#{ENV['ES_USERNAME']}"
  password "#{ENV['ES_PASSWORD']}"
  scheme https
  ssl_verify true
  ssl_version TLSv1_2
  
  logstash_format true
  logstash_prefix access-logs
  include_tag_key true
  tag_key @log_name
  
  <buffer>
    @type file
    path /var/log/fluentd/buffer/access-es
    flush_interval 5s
    retry_max_interval 30s
    retry_forever false
    retry_max_times 17
    flush_thread_count 4
  </buffer>
  
  <secondary>
    @type file
    path /var/log/fluentd/failed/access-es
    compress gzip
  </secondary>
</match>

# Audit logs to Elasticsearch
<match audit.**>
  @type elasticsearch
  host elasticsearch.monitoring.svc
  port 9200
  user "#{ENV['ES_USERNAME']}"
  password "#{ENV['ES_PASSWORD']}"
  scheme https
  ssl_verify true
  ssl_version TLSv1_2
  
  logstash_format true
  logstash_prefix audit-logs
  include_tag_key true
  tag_key @log_name
  
  <buffer>
    @type file
    path /var/log/fluentd/buffer/audit-es
    flush_interval 5s
    retry_max_interval 30s
    retry_forever false
    retry_max_times 17
    flush_thread_count 4
  </buffer>
  
  <secondary>
    @type file
    path /var/log/fluentd/failed/audit-es
    compress gzip
  </secondary>
</match>

# System logs to Elasticsearch
<match system.**>
  @type elasticsearch
  host elasticsearch.monitoring.svc
  port 9200
  user "#{ENV['ES_USERNAME']}"
  password "#{ENV['ES_PASSWORD']}"
  scheme https
  ssl_verify true
  ssl_version TLSv1_2
  
  logstash_format true
  logstash_prefix system-logs
  include_tag_key true
  tag_key @log_name
  
  <buffer>
    @type file
    path /var/log/fluentd/buffer/system-es
    flush_interval 5s
    retry_max_interval 30s
    retry_forever false
    retry_max_times 17
    flush_thread_count 4
  </buffer>
  
  <secondary>
    @type file
    path /var/log/fluentd/failed/system-es
    compress gzip
  </secondary>
</match>

# =================================================================
# S3 ARCHIVING
# =================================================================
# Archive application logs - 30 days active, 1 year archived
<match app.**>
  @type copy
  <store>
    @type s3
    aws_key_id "#{ENV['AWS_ACCESS_KEY_ID']}"
    aws_sec_key "#{ENV['AWS_SECRET_ACCESS_KEY']}"
    s3_bucket task-management-logs
    s3_region us-east-1
    path logs/app/%Y/%m/%d/
    s3_object_key_format %{path}%{time_slice}_%{index}.%{file_extension}
    time_slice_format %Y%m%d-%H
    
    <buffer time>
      @type file
      path /var/log/fluentd/buffer/app-s3
      timekey 3600             # 1 hour
      timekey_wait 10m         # Wait 10 minutes before shipping
      flush_mode interval
      flush_interval 30m       # Flush every 30 minutes
      chunk_limit_size 256m
      total_limit_size 4g
    </buffer>
    
    <format>
      @type json
    </format>
    
    <secondary>
      @type file
      path /var/log/fluentd/failed/app-s3
      compress gzip
    </secondary>
    
    # Use GZIP compression
    store_as gzip
  </store>
</match>

# Archive access logs - 90 days
<match access.**>
  @type copy
  <store>
    @type s3
    aws_key_id "#{ENV['AWS_ACCESS_KEY_ID']}"
    aws_sec_key "#{ENV['AWS_SECRET_ACCESS_KEY']}"
    s3_bucket task-management-logs
    s3_region us-east-1
    path logs/access/%Y/%m/%d/
    s3_object_key_format %{path}%{time_slice}_%{index}.%{file_extension}
    time_slice_format %Y%m%d-%H
    
    <buffer time>
      @type file
      path /var/log/fluentd/buffer/access-s3
      timekey 3600
      timekey_wait 10m
      flush_mode interval
      flush_interval 30m
      chunk_limit_size 256m
      total_limit_size 4g
    </buffer>
    
    <format>
      @type json
    </format>
    
    <secondary>
      @type file
      path /var/log/fluentd/failed/access-s3
      compress gzip
    </secondary>
    
    store_as gzip
  </store>
</match>

# Archive audit logs - 1 year
<match audit.**>
  @type copy
  <store>
    @type s3
    aws_key_id "#{ENV['AWS_ACCESS_KEY_ID']}"
    aws_sec_key "#{ENV['AWS_SECRET_ACCESS_KEY']}"
    s3_bucket task-management-logs
    s3_region us-east-1
    path logs/audit/%Y/%m/%d/
    s3_object_key_format %{path}%{time_slice}_%{index}.%{file_extension}
    time_slice_format %Y%m%d-%H
    
    <buffer time>
      @type file
      path /var/log/fluentd/buffer/audit-s3
      timekey 3600
      timekey_wait 10m
      flush_mode interval
      flush_interval 30m
      chunk_limit_size 256m
      total_limit_size 4g
    </buffer>
    
    <format>
      @type json
    </format>
    
    <secondary>
      @type file
      path /var/log/fluentd/failed/audit-s3
      compress gzip
    </secondary>
    
    store_as gzip
  </store>
</match>

# Archive system logs - 30 days
<match system.**>
  @type copy
  <store>
    @type s3
    aws_key_id "#{ENV['AWS_ACCESS_KEY_ID']}"
    aws_sec_key "#{ENV['AWS_SECRET_ACCESS_KEY']}"
    s3_bucket task-management-logs
    s3_region us-east-1
    path logs/system/%Y/%m/%d/
    s3_object_key_format %{path}%{time_slice}_%{index}.%{file_extension}
    time_slice_format %Y%m%d-%H
    
    <buffer time>
      @type file
      path /var/log/fluentd/buffer/system-s3
      timekey 3600
      timekey_wait 10m
      flush_mode interval
      flush_interval 30m
      chunk_limit_size 256m
      total_limit_size 4g
    </buffer>
    
    <format>
      @type json
    </format>
    
    <secondary>
      @type file
      path /var/log/fluentd/failed/system-s3
      compress gzip
    </secondary>
    
    store_as gzip
  </store>
</match>

# =================================================================
# ALERTING FOR CRITICAL LOGS
# =================================================================
# Filter for error and critical logs
<filter app.**>
  @type grep
  <regexp>
    key severity
    pattern /(error|critical|emergency|alert)/i
  </regexp>
</filter>

# Send critical logs to AlertManager
<match app.**>
  @type copy
  <store>
    @type http
    endpoint http://alertmanager.monitoring.svc:9093/api/v1/alerts
    open_timeout 2
    
    <format>
      @type json
    </format>
    
    <buffer>
      @type memory
      flush_interval 10s
      retry_max_interval 30s
      retry_forever false
      retry_max_times 5
      chunk_limit_size 1m
    </buffer>
    
    <secondary>
      @type file
      path /var/log/fluentd/failed/alerts
      compress gzip
    </secondary>
    
    # Transform logs to AlertManager format
    <inject>
      time_key time
      time_type string
      time_format %Y-%m-%dT%H:%M:%S%z
    </inject>
    
    # Convert to AlertManager format
    # This is simplified and would need proper transformation
    json_builder do |record|
      {
        "labels": {
          "alertname": "ApplicationError",
          "severity": record["severity"],
          "service": record["service_name"],
          "environment": record["environment"]
        },
        "annotations": {
          "summary": "Error in #{record['service_name']}",
          "description": record["message"]
        },
        "startsAt": record["timestamp"]
      }
    end
  </store>
</match>

# =================================================================
# CATCH-ALL FOR MISSED LOGS
# =================================================================
<match **>
  @type file
  path /var/log/fluentd/unmatched/unmatched.log
  append true
  <buffer>
    @type file
    path /var/log/fluentd/buffer/unmatched
    flush_mode interval
    flush_interval 30s
  </buffer>
</match>